{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004122,
     "end_time": "2022-07-06T15:09:30.795327",
     "exception": false,
     "start_time": "2022-07-06T15:09:30.791205",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Lag Features Are All You Need\n",
    "\n",
    "> **Credits:** Based on [this](https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7963) amazing notebook.\n",
    "\n",
    "OK. Maybe not **all** you need.\n",
    "<br>\n",
    "But they improve `LightGBM`!\n",
    "_____\n",
    "\n",
    "\n",
    "This notebook stated as an ensemble of `LightGBM` + `Catboost` + `XGB` but while running it I discovered an interestin idea that worked really well.\n",
    "\n",
    "### Lag Features\n",
    "\n",
    "On this competition we get information about clients of AMEX over time. \n",
    "Most high scoring notebooks on this competiion focused on aggregating the information per client and create a single row of extracted features: One for each client.\n",
    "\n",
    "**One of such agg function is `last`**.\n",
    "\n",
    "Quick examination revealed that the `last` feature is extreamly powerful at predicting if the client defaults or not (well.. make sense..). \n",
    "So I took this two steps further: \n",
    "\n",
    "- **First feature:** Just like the `last` feature: I added a `first` feature. \n",
    "- **\"Lag\" fearures:** to capture the change over time about each client I calculated two features for every `first`, `last` pair:\n",
    "     - **Last - First:** The change since we first see the client to the last time we see the client.\n",
    "     - **Last / First:** The fractional difference since we first see the client to the last time we see the client.\n",
    "\n",
    "This improved my `LightGBM` model to the point that it overtook the whole `LightGBM` + `Catboost` + `XGB` ensemble.\n",
    "\n",
    "I uploaded a dataset containing the extracted lag features and updated the final model predictions (only `LightGBM` this time) for everyone to play with. \n",
    "\n",
    "<br>\n",
    "\n",
    "_____\n",
    "\n",
    "**Next Experiement (currently running):** More \"lag features\" variations - Also take in consideration other indices of the time-series. will keep you updated.\n",
    "_____\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import time\n",
    "path_test = r'C:\\Users\\pavlo\\Desktop\\Kaggle\\AE\\AE_BEST_CHANCE\\test.parquet'\n",
    "#path_test = None\n",
    "path_train = r'C:\\Users\\pavlo\\Desktop\\Kaggle\\AE\\AE_BEST_CHANCE\\train.parquet'\n",
    "path_label = r'C:\\Users\\pavlo\\Desktop\\Kaggle\\AE\\train_labels.csv\\train_labels.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.002645,
     "end_time": "2022-07-06T15:09:30.803906",
     "exception": false,
     "start_time": "2022-07-06T15:09:30.801261",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-06T15:09:30.812870Z",
     "iopub.status.busy": "2022-07-06T15:09:30.812438Z",
     "iopub.status.idle": "2022-07-06T15:09:30.995590Z",
     "shell.execute_reply": "2022-07-06T15:09:30.994151Z"
    },
    "papermill": {
     "duration": 0.191822,
     "end_time": "2022-07-06T15:09:30.998606",
     "exception": false,
     "start_time": "2022-07-06T15:09:30.806784",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet(path_train)\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    \n",
    "    # Train FE\n",
    "    print('Starting train feature extraction')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in train_num_agg:\n",
    "        if 'last' in col and col.replace('last', 'first') in train_num_agg:\n",
    "            train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', 'first')]\n",
    "            train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', 'first')]\n",
    "\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    \n",
    "    start = time.time()\n",
    "    chunk = pd.read_csv(path_label,chunksize=1000000)\n",
    "    train_labels = pd.concat(chunk)\n",
    "    end = time.time()\n",
    "    print(\"Read csv with chunks: \",(end-start),\"sec\")\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    print('Train shape: ', train.shape)    \n",
    "    del train_num_agg, train_cat_agg        \n",
    "    gc.collect()\n",
    "    \n",
    "    # Test FE\n",
    "    test = pd.read_parquet(path_test)\n",
    "    print('Starting test feature extraction')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in test_num_agg:\n",
    "        if 'last' in col and col.replace('last', 'first') in test_num_agg:\n",
    "            test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', 'first')]\n",
    "            test_num_agg[col + '_lag_div'] = test_num_agg[col] / test_num_agg[col.replace('last', 'first')]\n",
    "\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    \n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID')\n",
    "    print('Test shape: ', test.shape)\n",
    "    del test_num_agg, test_cat_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    # Save files to disk\n",
    "    train.to_parquet('train_plus.parquet')\n",
    "    test.to_parquet('test_plus.parquet')\n",
    "    \n",
    "# Read & Preprocess Data\n",
    "# read_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train feature extraction\n",
      "Read csv with chunks:  0.30119991302490234 sec\n",
      "Train shape:  (458913, 1462)\n",
      "Starting test feature extraction\n",
      "Test shape:  (924621, 1461)\n"
     ]
    }
   ],
   "source": [
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.002685,
     "end_time": "2022-07-06T15:09:31.004476",
     "exception": false,
     "start_time": "2022-07-06T15:09:31.001791",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T15:09:31.014704Z",
     "iopub.status.busy": "2022-07-06T15:09:31.014115Z",
     "iopub.status.idle": "2022-07-06T15:09:33.679478Z",
     "shell.execute_reply": "2022-07-06T15:09:33.678164Z"
    },
    "papermill": {
     "duration": 2.673873,
     "end_time": "2022-07-06T15:09:33.682611",
     "exception": false,
     "start_time": "2022-07-06T15:09:31.008738",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "#import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from catboost import CatBoostClassifier\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "class CFG:\n",
    "    seed = 777\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_data():\n",
    "    path_test = r'C:\\Users\\pavlo\\Desktop\\Kaggle\\AE\\AE_BEST_CHANCE\\test_plus.parquet'\n",
    "    path_train = r'C:\\Users\\pavlo\\Desktop\\Kaggle\\AE\\AE_BEST_CHANCE\\train_plus.parquet'\n",
    "    #VALUE_NAN = \n",
    "    train = pd.read_parquet(path_train)#.fillna(VALUE_NAN)\n",
    "    test = pd.read_parquet(path_test)#.fillna(VALUE_NAN)\n",
    "    return train, test\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def amex_metric_np(preds, target):\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003511,
     "end_time": "2022-07-06T15:09:33.690195",
     "exception": false,
     "start_time": "2022-07-06T15:09:33.686684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training LightGBM (DART) Model\n",
    "\n",
    "- Final predictions output uploaded as a public dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T15:09:33.698380Z",
     "iopub.status.busy": "2022-07-06T15:09:33.697943Z",
     "iopub.status.idle": "2022-07-06T15:09:33.719006Z",
     "shell.execute_reply": "2022-07-06T15:09:33.717770Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028439,
     "end_time": "2022-07-06T15:09:33.721871",
     "exception": false,
     "start_time": "2022-07-06T15:09:33.693432",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        test[col + '_round2'] = test[col].round(2)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40\n",
    "        }\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, f'lgbm_fold{fold}_seed{CFG.seed}_plus.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'oof_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}_plus.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'test_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}_plus.csv', index = False)\n",
    "\n",
    "# seed_everything(CFG.seed)\n",
    "# train, test = read_data()\n",
    "# train_and_evaluate(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.706048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286808\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.323384\ttraining's amex_metric: 0.779366\tvalid_1's binary_logloss: 0.325789\tvalid_1's amex_metric: 0.768786\n",
      "[1000]\ttraining's binary_logloss: 0.255852\ttraining's amex_metric: 0.792964\tvalid_1's binary_logloss: 0.260988\tvalid_1's amex_metric: 0.777105\n",
      "[1500]\ttraining's binary_logloss: 0.223395\ttraining's amex_metric: 0.806499\tvalid_1's binary_logloss: 0.232673\tvalid_1's amex_metric: 0.783755\n",
      "[2000]\ttraining's binary_logloss: 0.209864\ttraining's amex_metric: 0.819966\tvalid_1's binary_logloss: 0.22392\tvalid_1's amex_metric: 0.787857\n",
      "[2500]\ttraining's binary_logloss: 0.200117\ttraining's amex_metric: 0.831797\tvalid_1's binary_logloss: 0.219632\tvalid_1's amex_metric: 0.790575\n",
      "[3000]\ttraining's binary_logloss: 0.193649\ttraining's amex_metric: 0.842404\tvalid_1's binary_logloss: 0.217923\tvalid_1's amex_metric: 0.79235\n",
      "[3500]\ttraining's binary_logloss: 0.187547\ttraining's amex_metric: 0.852562\tvalid_1's binary_logloss: 0.216599\tvalid_1's amex_metric: 0.794746\n",
      "[4000]\ttraining's binary_logloss: 0.182085\ttraining's amex_metric: 0.862562\tvalid_1's binary_logloss: 0.215757\tvalid_1's amex_metric: 0.795314\n",
      "[4500]\ttraining's binary_logloss: 0.176176\ttraining's amex_metric: 0.87142\tvalid_1's binary_logloss: 0.215017\tvalid_1's amex_metric: 0.796109\n",
      "[5000]\ttraining's binary_logloss: 0.170662\ttraining's amex_metric: 0.881091\tvalid_1's binary_logloss: 0.214446\tvalid_1's amex_metric: 0.797403\n",
      "[5500]\ttraining's binary_logloss: 0.16609\ttraining's amex_metric: 0.889541\tvalid_1's binary_logloss: 0.214101\tvalid_1's amex_metric: 0.79718\n",
      "[6000]\ttraining's binary_logloss: 0.161065\ttraining's amex_metric: 0.89723\tvalid_1's binary_logloss: 0.213789\tvalid_1's amex_metric: 0.797368\n",
      "[6500]\ttraining's binary_logloss: 0.156577\ttraining's amex_metric: 0.905357\tvalid_1's binary_logloss: 0.213567\tvalid_1's amex_metric: 0.797623\n",
      "[7000]\ttraining's binary_logloss: 0.152633\ttraining's amex_metric: 0.912213\tvalid_1's binary_logloss: 0.213441\tvalid_1's amex_metric: 0.797316\n",
      "[7500]\ttraining's binary_logloss: 0.147829\ttraining's amex_metric: 0.919643\tvalid_1's binary_logloss: 0.213161\tvalid_1's amex_metric: 0.797949\n",
      "[8000]\ttraining's binary_logloss: 0.144312\ttraining's amex_metric: 0.925793\tvalid_1's binary_logloss: 0.213107\tvalid_1's amex_metric: 0.797872\n",
      "[8500]\ttraining's binary_logloss: 0.139753\ttraining's amex_metric: 0.932716\tvalid_1's binary_logloss: 0.212988\tvalid_1's amex_metric: 0.797868\n",
      "[9000]\ttraining's binary_logloss: 0.136362\ttraining's amex_metric: 0.938358\tvalid_1's binary_logloss: 0.212977\tvalid_1's amex_metric: 0.797867\n",
      "[9500]\ttraining's binary_logloss: 0.132517\ttraining's amex_metric: 0.943764\tvalid_1's binary_logloss: 0.212862\tvalid_1's amex_metric: 0.798241\n",
      "[10000]\ttraining's binary_logloss: 0.129033\ttraining's amex_metric: 0.949125\tvalid_1's binary_logloss: 0.212892\tvalid_1's amex_metric: 0.797807\n",
      "[10500]\ttraining's binary_logloss: 0.125389\ttraining's amex_metric: 0.954078\tvalid_1's binary_logloss: 0.212862\tvalid_1's amex_metric: 0.798237\n",
      "Our fold 0 CV score is 0.7982366737945807\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.724058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286883\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.322986\ttraining's amex_metric: 0.779588\tvalid_1's binary_logloss: 0.32732\tvalid_1's amex_metric: 0.765827\n",
      "[1000]\ttraining's binary_logloss: 0.255296\ttraining's amex_metric: 0.793143\tvalid_1's binary_logloss: 0.262894\tvalid_1's amex_metric: 0.774786\n",
      "[1500]\ttraining's binary_logloss: 0.222802\ttraining's amex_metric: 0.806968\tvalid_1's binary_logloss: 0.234815\tvalid_1's amex_metric: 0.781024\n",
      "[2000]\ttraining's binary_logloss: 0.209237\ttraining's amex_metric: 0.820321\tvalid_1's binary_logloss: 0.226108\tvalid_1's amex_metric: 0.785622\n",
      "[2500]\ttraining's binary_logloss: 0.19949\ttraining's amex_metric: 0.832659\tvalid_1's binary_logloss: 0.221969\tvalid_1's amex_metric: 0.787883\n",
      "[3000]\ttraining's binary_logloss: 0.193029\ttraining's amex_metric: 0.843555\tvalid_1's binary_logloss: 0.220266\tvalid_1's amex_metric: 0.790885\n",
      "[3500]\ttraining's binary_logloss: 0.186922\ttraining's amex_metric: 0.853193\tvalid_1's binary_logloss: 0.219106\tvalid_1's amex_metric: 0.791175\n",
      "[4000]\ttraining's binary_logloss: 0.181486\ttraining's amex_metric: 0.862611\tvalid_1's binary_logloss: 0.218366\tvalid_1's amex_metric: 0.792416\n",
      "[4500]\ttraining's binary_logloss: 0.17556\ttraining's amex_metric: 0.872466\tvalid_1's binary_logloss: 0.217656\tvalid_1's amex_metric: 0.793292\n",
      "[5000]\ttraining's binary_logloss: 0.169999\ttraining's amex_metric: 0.881529\tvalid_1's binary_logloss: 0.217133\tvalid_1's amex_metric: 0.793412\n",
      "[5500]\ttraining's binary_logloss: 0.165466\ttraining's amex_metric: 0.889534\tvalid_1's binary_logloss: 0.216832\tvalid_1's amex_metric: 0.79328\n",
      "[6000]\ttraining's binary_logloss: 0.160456\ttraining's amex_metric: 0.897792\tvalid_1's binary_logloss: 0.216506\tvalid_1's amex_metric: 0.794559\n",
      "[6500]\ttraining's binary_logloss: 0.155963\ttraining's amex_metric: 0.905754\tvalid_1's binary_logloss: 0.216318\tvalid_1's amex_metric: 0.794067\n",
      "[7000]\ttraining's binary_logloss: 0.152019\ttraining's amex_metric: 0.912917\tvalid_1's binary_logloss: 0.216199\tvalid_1's amex_metric: 0.794356\n",
      "[7500]\ttraining's binary_logloss: 0.14722\ttraining's amex_metric: 0.91982\tvalid_1's binary_logloss: 0.216055\tvalid_1's amex_metric: 0.794234\n",
      "[8000]\ttraining's binary_logloss: 0.14367\ttraining's amex_metric: 0.926513\tvalid_1's binary_logloss: 0.21599\tvalid_1's amex_metric: 0.794323\n",
      "[8500]\ttraining's binary_logloss: 0.139111\ttraining's amex_metric: 0.93307\tvalid_1's binary_logloss: 0.215911\tvalid_1's amex_metric: 0.79461\n",
      "[9000]\ttraining's binary_logloss: 0.135743\ttraining's amex_metric: 0.939067\tvalid_1's binary_logloss: 0.215947\tvalid_1's amex_metric: 0.794284\n",
      "[9500]\ttraining's binary_logloss: 0.131921\ttraining's amex_metric: 0.944429\tvalid_1's binary_logloss: 0.215971\tvalid_1's amex_metric: 0.794255\n",
      "[10000]\ttraining's binary_logloss: 0.128459\ttraining's amex_metric: 0.949781\tvalid_1's binary_logloss: 0.215943\tvalid_1's amex_metric: 0.794073\n",
      "[10500]\ttraining's binary_logloss: 0.124822\ttraining's amex_metric: 0.954625\tvalid_1's binary_logloss: 0.215931\tvalid_1's amex_metric: 0.794357\n",
      "Our fold 1 CV score is 0.7943571856542313\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.657382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286723\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.323238\ttraining's amex_metric: 0.779013\tvalid_1's binary_logloss: 0.32648\tvalid_1's amex_metric: 0.766854\n",
      "[1000]\ttraining's binary_logloss: 0.255603\ttraining's amex_metric: 0.793297\tvalid_1's binary_logloss: 0.261757\tvalid_1's amex_metric: 0.775602\n",
      "[1500]\ttraining's binary_logloss: 0.223129\ttraining's amex_metric: 0.806784\tvalid_1's binary_logloss: 0.233445\tvalid_1's amex_metric: 0.782491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\ttraining's binary_logloss: 0.209659\ttraining's amex_metric: 0.820052\tvalid_1's binary_logloss: 0.224681\tvalid_1's amex_metric: 0.785802\n",
      "[2500]\ttraining's binary_logloss: 0.199869\ttraining's amex_metric: 0.832065\tvalid_1's binary_logloss: 0.220252\tvalid_1's amex_metric: 0.789768\n",
      "[3000]\ttraining's binary_logloss: 0.193431\ttraining's amex_metric: 0.842594\tvalid_1's binary_logloss: 0.218488\tvalid_1's amex_metric: 0.791777\n",
      "[3500]\ttraining's binary_logloss: 0.187336\ttraining's amex_metric: 0.85226\tvalid_1's binary_logloss: 0.217223\tvalid_1's amex_metric: 0.79364\n",
      "[4000]\ttraining's binary_logloss: 0.181924\ttraining's amex_metric: 0.862077\tvalid_1's binary_logloss: 0.21641\tvalid_1's amex_metric: 0.793965\n",
      "[4500]\ttraining's binary_logloss: 0.176029\ttraining's amex_metric: 0.871845\tvalid_1's binary_logloss: 0.215617\tvalid_1's amex_metric: 0.794979\n",
      "[5000]\ttraining's binary_logloss: 0.170496\ttraining's amex_metric: 0.880638\tvalid_1's binary_logloss: 0.215027\tvalid_1's amex_metric: 0.794615\n",
      "[5500]\ttraining's binary_logloss: 0.165956\ttraining's amex_metric: 0.889125\tvalid_1's binary_logloss: 0.214686\tvalid_1's amex_metric: 0.794493\n",
      "[6000]\ttraining's binary_logloss: 0.160907\ttraining's amex_metric: 0.897722\tvalid_1's binary_logloss: 0.214346\tvalid_1's amex_metric: 0.795496\n",
      "[6500]\ttraining's binary_logloss: 0.15643\ttraining's amex_metric: 0.90541\tvalid_1's binary_logloss: 0.214204\tvalid_1's amex_metric: 0.795698\n",
      "[7000]\ttraining's binary_logloss: 0.15249\ttraining's amex_metric: 0.912016\tvalid_1's binary_logloss: 0.214045\tvalid_1's amex_metric: 0.795677\n",
      "[7500]\ttraining's binary_logloss: 0.147713\ttraining's amex_metric: 0.919391\tvalid_1's binary_logloss: 0.21381\tvalid_1's amex_metric: 0.795332\n",
      "[8000]\ttraining's binary_logloss: 0.144196\ttraining's amex_metric: 0.925944\tvalid_1's binary_logloss: 0.213765\tvalid_1's amex_metric: 0.795767\n",
      "[8500]\ttraining's binary_logloss: 0.139629\ttraining's amex_metric: 0.932686\tvalid_1's binary_logloss: 0.213607\tvalid_1's amex_metric: 0.79553\n",
      "[9000]\ttraining's binary_logloss: 0.136229\ttraining's amex_metric: 0.938344\tvalid_1's binary_logloss: 0.213639\tvalid_1's amex_metric: 0.795664\n",
      "[9500]\ttraining's binary_logloss: 0.132373\ttraining's amex_metric: 0.943747\tvalid_1's binary_logloss: 0.2136\tvalid_1's amex_metric: 0.79565\n",
      "[10000]\ttraining's binary_logloss: 0.128885\ttraining's amex_metric: 0.949086\tvalid_1's binary_logloss: 0.213584\tvalid_1's amex_metric: 0.796052\n",
      "[10500]\ttraining's binary_logloss: 0.12527\ttraining's amex_metric: 0.954114\tvalid_1's binary_logloss: 0.213583\tvalid_1's amex_metric: 0.79637\n",
      "Our fold 2 CV score is 0.7963697860910237\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.774744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286729\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.323167\ttraining's amex_metric: 0.779591\tvalid_1's binary_logloss: 0.3265\tvalid_1's amex_metric: 0.767658\n",
      "[1000]\ttraining's binary_logloss: 0.255458\ttraining's amex_metric: 0.79256\tvalid_1's binary_logloss: 0.262192\tvalid_1's amex_metric: 0.776189\n",
      "[1500]\ttraining's binary_logloss: 0.22293\ttraining's amex_metric: 0.806835\tvalid_1's binary_logloss: 0.234236\tvalid_1's amex_metric: 0.783092\n",
      "[2000]\ttraining's binary_logloss: 0.209377\ttraining's amex_metric: 0.819656\tvalid_1's binary_logloss: 0.225619\tvalid_1's amex_metric: 0.786863\n",
      "[2500]\ttraining's binary_logloss: 0.199571\ttraining's amex_metric: 0.831751\tvalid_1's binary_logloss: 0.22137\tvalid_1's amex_metric: 0.78895\n",
      "[3000]\ttraining's binary_logloss: 0.193152\ttraining's amex_metric: 0.84272\tvalid_1's binary_logloss: 0.219689\tvalid_1's amex_metric: 0.789964\n",
      "[3500]\ttraining's binary_logloss: 0.187032\ttraining's amex_metric: 0.85218\tvalid_1's binary_logloss: 0.218447\tvalid_1's amex_metric: 0.791805\n",
      "[4000]\ttraining's binary_logloss: 0.18161\ttraining's amex_metric: 0.862292\tvalid_1's binary_logloss: 0.217721\tvalid_1's amex_metric: 0.79304\n",
      "[4500]\ttraining's binary_logloss: 0.175688\ttraining's amex_metric: 0.871932\tvalid_1's binary_logloss: 0.217011\tvalid_1's amex_metric: 0.794372\n",
      "[5000]\ttraining's binary_logloss: 0.170172\ttraining's amex_metric: 0.881021\tvalid_1's binary_logloss: 0.216528\tvalid_1's amex_metric: 0.794748\n",
      "[5500]\ttraining's binary_logloss: 0.165658\ttraining's amex_metric: 0.889358\tvalid_1's binary_logloss: 0.216277\tvalid_1's amex_metric: 0.795353\n",
      "[6000]\ttraining's binary_logloss: 0.160595\ttraining's amex_metric: 0.898054\tvalid_1's binary_logloss: 0.215969\tvalid_1's amex_metric: 0.796066\n",
      "[6500]\ttraining's binary_logloss: 0.156124\ttraining's amex_metric: 0.905519\tvalid_1's binary_logloss: 0.215811\tvalid_1's amex_metric: 0.796022\n",
      "[7000]\ttraining's binary_logloss: 0.152151\ttraining's amex_metric: 0.912404\tvalid_1's binary_logloss: 0.215644\tvalid_1's amex_metric: 0.796431\n",
      "[7500]\ttraining's binary_logloss: 0.147364\ttraining's amex_metric: 0.919911\tvalid_1's binary_logloss: 0.215487\tvalid_1's amex_metric: 0.795787\n",
      "[8000]\ttraining's binary_logloss: 0.143808\ttraining's amex_metric: 0.925889\tvalid_1's binary_logloss: 0.215437\tvalid_1's amex_metric: 0.795736\n",
      "[8500]\ttraining's binary_logloss: 0.139263\ttraining's amex_metric: 0.932873\tvalid_1's binary_logloss: 0.215317\tvalid_1's amex_metric: 0.796419\n",
      "[9000]\ttraining's binary_logloss: 0.135863\ttraining's amex_metric: 0.938657\tvalid_1's binary_logloss: 0.215223\tvalid_1's amex_metric: 0.796241\n",
      "[9500]\ttraining's binary_logloss: 0.132013\ttraining's amex_metric: 0.944568\tvalid_1's binary_logloss: 0.215164\tvalid_1's amex_metric: 0.7967\n",
      "[10000]\ttraining's binary_logloss: 0.128553\ttraining's amex_metric: 0.949453\tvalid_1's binary_logloss: 0.215179\tvalid_1's amex_metric: 0.796253\n",
      "[10500]\ttraining's binary_logloss: 0.124927\ttraining's amex_metric: 0.954472\tvalid_1's binary_logloss: 0.215205\tvalid_1's amex_metric: 0.79637\n",
      "Our fold 3 CV score is 0.7963695020042496\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.960798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286770\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.322783\ttraining's amex_metric: 0.780008\tvalid_1's binary_logloss: 0.327787\tvalid_1's amex_metric: 0.764483\n",
      "[1000]\ttraining's binary_logloss: 0.254922\ttraining's amex_metric: 0.793704\tvalid_1's binary_logloss: 0.263656\tvalid_1's amex_metric: 0.772782\n",
      "[1500]\ttraining's binary_logloss: 0.222395\ttraining's amex_metric: 0.80802\tvalid_1's binary_logloss: 0.235925\tvalid_1's amex_metric: 0.778104\n",
      "[2000]\ttraining's binary_logloss: 0.208886\ttraining's amex_metric: 0.821167\tvalid_1's binary_logloss: 0.227405\tvalid_1's amex_metric: 0.782425\n",
      "[2500]\ttraining's binary_logloss: 0.199189\ttraining's amex_metric: 0.833663\tvalid_1's binary_logloss: 0.223251\tvalid_1's amex_metric: 0.787283\n",
      "[3000]\ttraining's binary_logloss: 0.192728\ttraining's amex_metric: 0.844032\tvalid_1's binary_logloss: 0.221462\tvalid_1's amex_metric: 0.789842\n",
      "[3500]\ttraining's binary_logloss: 0.186609\ttraining's amex_metric: 0.853454\tvalid_1's binary_logloss: 0.220114\tvalid_1's amex_metric: 0.791081\n",
      "[4000]\ttraining's binary_logloss: 0.181154\ttraining's amex_metric: 0.863077\tvalid_1's binary_logloss: 0.219276\tvalid_1's amex_metric: 0.792109\n",
      "[4500]\ttraining's binary_logloss: 0.175266\ttraining's amex_metric: 0.872561\tvalid_1's binary_logloss: 0.218595\tvalid_1's amex_metric: 0.792153\n",
      "[5000]\ttraining's binary_logloss: 0.169732\ttraining's amex_metric: 0.881631\tvalid_1's binary_logloss: 0.218092\tvalid_1's amex_metric: 0.792635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5500]\ttraining's binary_logloss: 0.165201\ttraining's amex_metric: 0.889799\tvalid_1's binary_logloss: 0.217857\tvalid_1's amex_metric: 0.793542\n",
      "[6000]\ttraining's binary_logloss: 0.16017\ttraining's amex_metric: 0.898133\tvalid_1's binary_logloss: 0.217547\tvalid_1's amex_metric: 0.79408\n",
      "[6500]\ttraining's binary_logloss: 0.15568\ttraining's amex_metric: 0.905869\tvalid_1's binary_logloss: 0.217435\tvalid_1's amex_metric: 0.794\n",
      "[7000]\ttraining's binary_logloss: 0.151724\ttraining's amex_metric: 0.912699\tvalid_1's binary_logloss: 0.217299\tvalid_1's amex_metric: 0.79459\n",
      "[7500]\ttraining's binary_logloss: 0.14694\ttraining's amex_metric: 0.920081\tvalid_1's binary_logloss: 0.217162\tvalid_1's amex_metric: 0.794706\n",
      "[8000]\ttraining's binary_logloss: 0.14341\ttraining's amex_metric: 0.926748\tvalid_1's binary_logloss: 0.217097\tvalid_1's amex_metric: 0.794329\n",
      "[8500]\ttraining's binary_logloss: 0.138866\ttraining's amex_metric: 0.93358\tvalid_1's binary_logloss: 0.216997\tvalid_1's amex_metric: 0.795098\n",
      "[9000]\ttraining's binary_logloss: 0.135496\ttraining's amex_metric: 0.939\tvalid_1's binary_logloss: 0.216985\tvalid_1's amex_metric: 0.794261\n",
      "[9500]\ttraining's binary_logloss: 0.13166\ttraining's amex_metric: 0.944761\tvalid_1's binary_logloss: 0.216954\tvalid_1's amex_metric: 0.793705\n",
      "[10000]\ttraining's binary_logloss: 0.128182\ttraining's amex_metric: 0.949959\tvalid_1's binary_logloss: 0.216969\tvalid_1's amex_metric: 0.79343\n",
      "[10500]\ttraining's binary_logloss: 0.124566\ttraining's amex_metric: 0.954878\tvalid_1's binary_logloss: 0.217007\tvalid_1's amex_metric: 0.793634\n",
      "Our fold 4 CV score is 0.7936337705959893\n",
      "Our out of folds CV score is 0.7956291129597466\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n",
    "train_and_evaluate(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.738938,
   "end_time": "2022-07-06T15:09:40.000109",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-06T15:09:20.261171",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
